{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63abd82c93392567",
   "metadata": {
    "id": "63abd82c93392567"
   },
   "source": [
    "## BIOENG-310: Neuroscience Foundations for Engineers\n",
    "\n",
    "Notebook created by Martin Schrimpf, edited by Alejandro Rodriguez Guajardo and Yingtian Tang.\n",
    "\n",
    "# Week 5: Data visualization and linear probing for V1 neural activities\n",
    "\n",
    "This week, we **visualize neural firing data** from the primary visual cortex (V1) of a macaque [(Freeman & Ziemba, 2013)](https://www.nature.com/articles/nn.3402). V1 is known to encode low-level features of visual stimuli, such as texture. To assess this, we apply a technique known as **linear readout** or **linear probing**. This method involves training a simple linear classifier, such as logistic regression or a support vector machine, on neural activity to predict stimulus features.\n",
    "\n",
    "The exercise is written in this *jupyter notebook*, which is a notebook of interleaved [Markdown](https://jupyter-notebook.readthedocs.io/en/latest/notebook.html#markdown-cells) and [Python blocks](https://jupyter-notebook.readthedocs.io/en/latest/notebook.html#code-cells). All of the blocks can be executed: Markdown blocks turn into text (so when they look strange, just execute them); Python blocks execute the code while keeping the variables so they can be used in other Python blocks. You can check more information on the basics of jupyter notebook [starting from this section of the whole webpage](https://jupyter-notebook.readthedocs.io/en/latest/notebook.html#notebook-user-interface).\n",
    "\n",
    "The notebook consists of the following sessions:\n",
    "- **Load Data**: load the experimental V1 neural data\n",
    "- **Stimuli**: load the stimuli used for the recording experiment\n",
    "- **Data visualization**: visualize the timeseries of V1 firing patterns (lots of work)\n",
    "- **Linear readout / linear probing**: use V1 activities to predict the texture of stimuli (lots of work)\n",
    "\n",
    "Please complete the code in the sections marked '''%%...%%''' based on the given instructions and context. Then, run all the blocks to see the results.\n",
    "Additional hints:\n",
    "- You can open some temporary Python blocks as your scratchpad.\n",
    "- Be careful of the ordering of running Python blocks, since the variables are shared.\n",
    "- References are important: look at the input/output specifications of the functions.\n",
    "\n",
    "Enjoy the journey! ðŸš€\n",
    "\n",
    "**Before you begin**, make sure you have all dependencies installed (see `requirements.txt` in this week's folder). You can install them by executing the following block. (The following code might require you to *restart runtime*, which is fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8728c240",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8728c240",
    "outputId": "649866c0-3249-4d22-b1e8-c12511024f04"
   },
   "outputs": [],
   "source": [
    "!pip install jupyter brainscore-vision matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5521fef852ac37",
   "metadata": {
    "id": "ec5521fef852ac37"
   },
   "source": [
    "### Load Data\n",
    "\n",
    "At the current state, it can unfortunately often be difficult to access brain recordings. Not all groups share their data (although new regulations are improving this), and even if the data is accessible it is often difficult to interact with due to non-standard file formats, missing/unclear metadata, and obscure processing pipelines.\n",
    "\n",
    "That being said, we will here save you the headache by starting from a packaged and standardized dataset.\n",
    "This data was first published by [Freeman* & Ziemba* et al. 2013](https://www.nature.com/articles/nn.3402) and subsequently packaged into the **[Brain-Score](www.brain-score.org)** platform.\n",
    "The format of the data here is in [xarray](https://xarray.dev), a structure allowing for multi-dimensional data with multiple metadata along all dimensions. If you know pandas, xarray is the multi-dimensional extension of it. Check basics of xarray [here](https://tutorial.xarray.dev/overview/xarray-in-45-min.html) and pandas [here](https://www.w3schools.com/python/pandas/default.asp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76164adba6a5f9d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T12:54:43.664318Z",
     "start_time": "2025-02-19T12:54:38.719754Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "76164adba6a5f9d5",
    "outputId": "9e3d70ac-1a3d-4c80-de0d-2ffe22cfc7aa"
   },
   "outputs": [],
   "source": [
    "import brainscore_vision\n",
    "\n",
    "# brainscore will download the data for you\n",
    "data = brainscore_vision.load_dataset('FreemanZiemba2013.public')\n",
    "\n",
    "# we'll focus on only V1 recordings in this exercise\n",
    "v1_data = data.sel(region='V1')\n",
    "\n",
    "# By just typing the name of a variable, jupyter will show its content.\n",
    "# In this case, the xarray 'v1_data' will be presented with (scroll in the below to have a full view; toggle on the left to show/hide them):\n",
    "#   1. basics of this data structure: bytes, shape, data type, etc.\n",
    "#   2. a graphical representation of its different dimensions\n",
    "#   3. a section called 'coordinates' that shows the names of the dimensions and their associated values\n",
    "v1_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0719685ddfbe556",
   "metadata": {
    "id": "f0719685ddfbe556"
   },
   "source": [
    "Let's get a basic understanding of the neural recording by examining the *coordinates*. Answer the following questions:\n",
    "\n",
    "How many presentation trials?\n",
    "\n",
    "How many neural sites?\n",
    "\n",
    "The temporal resolution of the recording is 1 ms per time bin. Then, how long is each trial?\n",
    "\n",
    "Each *neural site* is recorded using a single quartz-platinum-tungsten microelectrode implanted in the macaque brain. Do you think each neural site records the activity of a single neuron, or could it capture signals from multiple neurons? Explain your reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936050a427bc284f",
   "metadata": {
    "id": "936050a427bc284f"
   },
   "source": [
    "### Stimuli\n",
    "\n",
    "How did this data come about in the first place? Primate subjects were presented with images while experimenters were recording from early visual cortex.\n",
    "\n",
    "What are those images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2ed4964dcdaf46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T12:54:44.906597Z",
     "start_time": "2025-02-19T12:54:43.664318Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "5c2ed4964dcdaf46",
    "outputId": "939da2ea-a113-456b-e3e8-841752f8b1fb"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot, image\n",
    "\n",
    "# The xarray 'v1_data' has 3 attributes (accesse all of them by 'v1_data.attrs')\n",
    "#   1. stimulus_set_identifier: name of the stimulus set\n",
    "#   2. stimulus_set: a brainscore StimulusSet (a subclass of pandas.Dataframe) that records all the information about the stimulus set\n",
    "#   3. identifier: name of the whole experiment\n",
    "stimuli = v1_data.stimulus_set\n",
    "\n",
    "# a shortcut to access the *first* stimulus is in the stimulus set\n",
    "single_stimulus_id = stimuli['stimulus_id'].values[0]\n",
    "# note that the 'stimulus_id's are also present in the coordinates of the xarray 'v1_data'.\n",
    "# however, generally those ones are in arbitrary order and the same stimlus_id can be repeated.\n",
    "# therefore, it is better to access the 'stimulus_id' from the stimulus set.\n",
    "\n",
    "# use a method 'get_stimulus' of StimulusSet to get the stimulus storage path given the id\n",
    "image_path = stimuli.get_stimulus(single_stimulus_id)\n",
    "\n",
    "# show the image using matplotlib\n",
    "image_content = image.imread(image_path)\n",
    "pyplot.imshow(image_content, cmap='gray')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf46113d86642c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T12:54:45.553893Z",
     "start_time": "2025-02-19T12:54:44.909454Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "id": "9bf46113d86642c2",
    "outputId": "ebed8340-08a8-49ab-c028-bace734c7039"
   },
   "outputs": [],
   "source": [
    "# Let's look at some more stimuli.\n",
    "fig, axes = pyplot.subplots(nrows=1, ncols=10, figsize=(30, 3))\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    \n",
    "    # loop through multiple stimuli, retrieve their IDs, get their image paths, load the images, and display them in grayscale\n",
    "    # we now focus on the i-th stimulus \n",
    "    image_path = \"\"\"%% Retrive the image path of the i-th stimulus %%\"\"\"\n",
    "    image_content = image.imread(image_path)\n",
    "    ax.imshow(image_content, cmap='gray')\n",
    "    ax.set_axis_off()\n",
    "    \n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d50dcaa890afba0",
   "metadata": {
    "id": "7d50dcaa890afba0"
   },
   "source": [
    "### Data visualization\n",
    "\n",
    "Now that we've seen the images shown to the macaques, let's check the neural activity recorded in V1. It's always a good idea to first plot the raw activity data before running any complex analysis. This helps ensure the data looks as expected and there are no obvious issues.\n",
    "\n",
    "Let's start with the response to the single stimulus that we visualized in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8203159a205db3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T12:54:45.662904Z",
     "start_time": "2025-02-19T12:54:45.553893Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5e8203159a205db3",
    "outputId": "d2c251da-8273-4cc0-ccba-87f3c40f0b16"
   },
   "outputs": [],
   "source": [
    "single_stimulus_id = '''%% Again, retrieve the first stimulus ID from the dataset. %%'''\n",
    "\n",
    "# xarray.sel method allows slicing the currency xarray along one/multiple coordinate(s) or dimension(s)\n",
    "# reference: https://docs.xarray.dev/en/latest/generated/xarray.DataArray.sel.html\n",
    "# hint: you can select along either coordinate(s) or dimension(s)\n",
    "stimulus_data = v1_data.sel('''%% Select the data for the chosen stimulus. %%''')\n",
    "\n",
    "# before executing, think about the shape/coordinates of this xarray\n",
    "stimulus_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a6393c819c684e",
   "metadata": {
    "id": "33a6393c819c684e"
   },
   "source": [
    "We see that this single stimulus was shown a total of 20 times (repetitions), has 205 neural sites recorded, and was collected over 300 time-bins (in this case 1 ms each).\n",
    "In visual neuroscience, repeated trials are usually conducted to average out noise. Let's do just that by averaging all the repetitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08f7a75829982b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T12:54:45.837280Z",
     "start_time": "2025-02-19T12:54:45.662904Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "f08f7a75829982b7",
    "outputId": "35e5031a-bb4c-416f-fd26-9144a785c2f5"
   },
   "outputs": [],
   "source": [
    "from brainscore_vision.benchmark_helpers.neural_common import average_repetition\n",
    "\n",
    "# average over repeated repetitions of the same stimulus using the brainscore function\n",
    "averaged_stimulus_data = average_repetition(stimulus_data)\n",
    "\n",
    "# remove (or 'squeeze') the singleton dimension after the averaging along it\n",
    "averaged_stimulus_data = averaged_stimulus_data.squeeze('presentation')\n",
    "\n",
    "# before executing, think about the shape/coordinates of this xarray\n",
    "averaged_stimulus_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac831b75189ee57",
   "metadata": {
    "id": "cac831b75189ee57"
   },
   "source": [
    "Now we visualize the averaged recordings for this specific stimulus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9795ac412d0c21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T12:54:47.311991Z",
     "start_time": "2025-02-19T12:54:45.837280Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "id": "ce9795ac412d0c21",
    "outputId": "55920ca2-f867-400e-b7c8-d38ce6819e04"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "fig, ax = pyplot.subplots()\n",
    "\n",
    "# Construct a colormap that visualize the spiking activity of the neuron\n",
    "colormap = ax.imshow(averaged_stimulus_data.transpose('neuroid_id', 'time_bin').values, cmap='gray')\n",
    "\n",
    "def add_colorbar(fig, ax, colormap):\n",
    "    divider = make_axes_locatable(ax)  # make space for colorbar\n",
    "    ax_colorbar = divider.append_axes('right', size='5%', pad=0.05)\n",
    "    colorbar = fig.colorbar(colormap, cax=ax_colorbar)\n",
    "    colorbar.set_label('''%% Provide a label describing what the color represents %%''')\n",
    "\n",
    "add_colorbar(fig, ax, colormap)\n",
    "\n",
    "# %% Label the axes %%\n",
    "ax.set_xlabel('''%% Label the x-axis appropriately %%''')\n",
    "ax.set_ylabel('''%% Label the y-axis appropriately %%''')\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e83118eadbf3f4",
   "metadata": {
    "id": "93e83118eadbf3f4"
   },
   "source": [
    "Cool! Lots of activity. But always think of what the 'time' and 'neural site' dimension mean. This will be helpful.\n",
    "\n",
    "This was for one single stimulus, let's see what the data looks like on average across all the stimuli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0DFG_iL3Nf6b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "id": "0DFG_iL3Nf6b",
    "outputId": "f00abbd1-95ae-439a-a5de-fc34a48944d3"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "fig, ax = pyplot.subplots()\n",
    "\n",
    "# average over all stimuli, all repetitions\n",
    "# reference: https://docs.xarray.dev/en/stable/generated/xarray.DataArray.mean.html\n",
    "averaged_v1_data = v1_data.mean('''%% Specify the correct dimension to average over %%''')\n",
    "\n",
    "colormap = ax.imshow(averaged_v1_data.transpose('''%% Fill in the dimensions in the order you prefer %%''').values, cmap='gray')\n",
    "add_colorbar(fig, ax, colormap)\n",
    "ax.set_xlabel('''%% Label the x-axis appropriately %%''')\n",
    "ax.set_ylabel('''%% Label the y-axis appropriately %%''')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gYllERiONkAU",
   "metadata": {
    "id": "gYllERiONkAU"
   },
   "source": [
    "There is another way to show these activity patterns: plotting them as timeseries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e32ee09a3a59335",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T12:54:51.762003Z",
     "start_time": "2025-02-19T12:54:49.576412Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "7e32ee09a3a59335",
    "outputId": "059976c4-e790-42cc-a45a-43ffd3e18e8e"
   },
   "outputs": [],
   "source": [
    "averaged_v1_data.load()  # make sure all data is loaded (sometimes the xarray is lazily loaded)\n",
    "\n",
    "fig, ax = pyplot.subplots()\n",
    "time_bins = averaged_v1_data['time_bin_start'].values\n",
    "\n",
    "# plot individual sites\n",
    "neural_sites = averaged_v1_data['neuroid_id'].values\n",
    "for neural_site in neural_sites:\n",
    "\n",
    "    # here we demonstrate an example of multi-slicing in xarray, even if in this case it is not necessary.\n",
    "    # xarray.sel does not directly support slicing with multiple values, but this can be achieved in two ways:\n",
    "    match_site = [neuroid_id == neural_site for neuroid_id in neural_sites]  # a binary mask\n",
    "    match_idx = [i for i, neuroid_id in enumerate(neural_sites) if neuroid_id == neural_site]  # a list of indices\n",
    "    \n",
    "    # the *first* way:\n",
    "    site_data = averaged_v1_data[{'neuroid_id': match_site}]\n",
    "    # or\n",
    "    site_data = averaged_v1_data[{'neuroid_id': match_idx}]\n",
    "\n",
    "    # the *second* way:\n",
    "    site_data = averaged_v1_data.isel(neuroid_id=match_site)\n",
    "    # or\n",
    "    site_data = averaged_v1_data.isel(neuroid_id=match_idx)\n",
    "\n",
    "    # more information on xarray slicing at: https://docs.xarray.dev/en/latest/user-guide/indexing.html#vectorized-indexing\n",
    "\n",
    "    site_data = site_data.squeeze()\n",
    "    ax.plot(time_bins, site_data.values, color='gray', alpha=0.3)\n",
    "\n",
    "# also plot site average\n",
    "site_average = averaged_v1_data.mean('''%% Compute the average firing rate time series across all sites. %%''')\n",
    "ax.plot(time_bins, site_average.values, color='darkgray')\n",
    "\n",
    "# and the sites that are more active -- let's say the sites that at some point are active above 0.05\n",
    "active_sites = (averaged_v1_data > 0.05).any('time_bin')\n",
    "active_sites = averaged_v1_data[{'neuroid_id': active_sites}].mean('neuroid_id')\n",
    "ax.plot(time_bins, active_sites.values, color='black')\n",
    "\n",
    "ax.set_xlabel('Time (ms)')\n",
    "ax.set_ylabel('Normalized firing rate')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfccefe3ecf425b4",
   "metadata": {
    "id": "cfccefe3ecf425b4"
   },
   "source": [
    "What happens at time=0ms?\n",
    "\n",
    "When are most neural sites start to become active?\n",
    "\n",
    "Why does it take time for sites to become active?\n",
    "\n",
    "You guess: why is there activity again at around 270ms?\n",
    "\n",
    "You guess: why do some sites show small activity (e.g. towards the bottom)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba3ef99767c5d1a",
   "metadata": {
    "id": "eba3ef99767c5d1a"
   },
   "source": [
    "#### Bonus: Reproducing a figure from the paper\n",
    "\n",
    "In the original [paper](https://www.nature.com/articles/nn.3402) we took the data from, Figure 2b identifies a contrast of neural responses to noise and naturalistic stimuli between neural sites in V1 and V2.\n",
    "In other words, the neural activity shows different patterns across different stimulus types: texture *vs.* noise.\n",
    "\n",
    "With our data processing so far, we can reproduce this figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b6ec0743e13f82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T12:54:57.931154Z",
     "start_time": "2025-02-19T12:54:51.762003Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "c6b6ec0743e13f82",
    "outputId": "03863463-a792-4a17-f702-fb0ae126eaf7"
   },
   "outputs": [],
   "source": [
    "from brainio.assemblies import NeuronRecordingAssembly\n",
    "\n",
    "full_data = brainscore_vision.load_dataset('FreemanZiemba2013.public')  # make sure we have both the V1 and V2 data again\n",
    "\n",
    "def process_data(data: NeuronRecordingAssembly) -> tuple[NeuronRecordingAssembly, NeuronRecordingAssembly]:\n",
    "    data = data.mean('neuroid_id')\n",
    "    # note that in the paper these values are further normalized by each site's maximum firing rate\n",
    "    data.load()  # make sure data is loaded\n",
    "    texture_data = data.sel(texture_type='texture').mean('presentation')\n",
    "    noise_data = data.sel(texture_type='noise').mean('presentation')\n",
    "    return texture_data, noise_data\n",
    "\n",
    "v1_data = '''%% Extract only the regional data of V1 using .sel() %%'''\n",
    "v2_data = '''%% Extract only the regional data of V2 using .sel() %%'''\n",
    "\n",
    "fig, axes = pyplot.subplots(nrows=2)\n",
    "\n",
    "for title, ax, data, base_color in zip(\n",
    "        ['V1', 'V2'], axes, [v1_data, v2_data], ['green', 'blue']):\n",
    "    texture_data, noise_data = process_data(data)\n",
    "    time_bins = data['time_bin_start'].values\n",
    "    ax.plot(time_bins, texture_data.values, color=f\"dark{base_color}\", label='texture')\n",
    "    ax.plot(time_bins, noise_data.values, color=f\"light{base_color}\", label='noise')\n",
    "    ax.legend()\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel('Normalized firing rate')\n",
    "    if title == 'V2':\n",
    "        ax.set_xlabel('Time (ms)')\n",
    "    else:\n",
    "        ax.xaxis.set_ticklabels([])  # hide tick labels on top plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553842be",
   "metadata": {},
   "source": [
    "What do you see? Compare yours to the original figures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7ad7505180012d",
   "metadata": {
    "id": "db7ad7505180012d"
   },
   "source": [
    "### Linear readout / linear probing\n",
    "\n",
    "Since neural firing patterns vary with different stimuli, we want to understand what information these patterns truly encode about the stimuli.\n",
    "\n",
    "A very common technique to investigate the informational content of data is to see if a linear regression can predict some variables of interests.\n",
    "\n",
    "In our case, we're interested in whether V1 activity can predict if the stimuli contain texture or if they are simply pure noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397eab32b1225c9f",
   "metadata": {
    "id": "397eab32b1225c9f"
   },
   "source": [
    "#### Time-averaging\n",
    "\n",
    "To make things simpler, we will average out the time dimension from now on.\n",
    "\n",
    "In other words, we assume that the time-averaged activity magnitude, without considering dynamics, already encodes texture information.\n",
    "\n",
    "As we observed before, most of the interesting signal is localized to 50-200ms. Let's average out the time dimension in that range so that we can focus on this signal. We also again average over the repetitions of each stimulus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf968906d9e28df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T12:54:58.777012Z",
     "start_time": "2025-02-19T12:54:57.931154Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1cf968906d9e28df",
    "outputId": "ae0d9326-9af0-4640-d1e6-deb293ec0d29"
   },
   "outputs": [],
   "source": [
    "from brainscore_vision.benchmark_helpers.neural_common import average_repetition\n",
    "\n",
    "def average_time_range(data, time_window):\n",
    "    data = '''%% Select only the time bins within the given time_window %%'''\n",
    "    data = '''%% Compute the mean over the time dimension %%'''\n",
    "    # note that you can pass keep_attrs=True to keep the 'attrs' of the xarray for some xarray operations\n",
    "    return data\n",
    "\n",
    "repetition_average = average_repetition(v1_data)\n",
    "time_average = average_time_range(repetition_average, time_window = (50, 200))\n",
    "time_average"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3de3caf28f0d76",
   "metadata": {
    "id": "bf3de3caf28f0d76"
   },
   "source": [
    "Now without the time dimension, we're left with the responses 102 V1 neural sites to 135 stimuli.\n",
    "\n",
    "We can visualize this again using an 'imshow' plot, but please note that the axes have changed compared to the previous plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7564c268b9fc89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T12:55:01.020424Z",
     "start_time": "2025-02-19T12:54:58.777012Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "id": "9c7564c268b9fc89",
    "outputId": "b94d7a5b-66ea-465f-fcb8-c7592c10fb76"
   },
   "outputs": [],
   "source": [
    "fig, ax = pyplot.subplots()\n",
    "colormap = ax.imshow(time_average.values, cmap='Greens')\n",
    "add_colorbar(fig, ax, colormap)\n",
    "ax.set_xlabel('Stimuli (image #)')\n",
    "ax.set_ylabel('Neural site (#)')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd1b992d3f3298a",
   "metadata": {
    "id": "cfd1b992d3f3298a"
   },
   "source": [
    "Arguably it's not super obvious how much signal there is that is specific to images. We will analyze this in detail in the next few steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OyfUz0HPQkVA",
   "metadata": {
    "id": "OyfUz0HPQkVA"
   },
   "source": [
    "#### Predicting/decoding the texture types\n",
    "\n",
    "For this dataset, there are two types of stimuli: 'noise' images and 'texture' images. Let's see if we can differentiate among them using the V1 neural data.\n",
    "\n",
    "Concretely, we fit a classifier that predicts the texture type based on the neural firing rates.\n",
    "We want to see if we can really *predict* unseen data. Therefore, we split our data into a `train` and a `validation` split. We train the classifier only on the `train` split. Then, we check if the trained classifier has high accuracy on the `validation` split.\n",
    "\n",
    "The scikit-learn package has some great utilities for such simple classic machine learning tools.\n",
    "\n",
    "Note: In research, we would also use an additional `test` split that we hold out altogether and only analyze at the very end when all model parameters are locked in, to avoid implicit overfitting to the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f761ae87b4d33c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T12:55:05.964918Z",
     "start_time": "2025-02-19T12:55:01.020424Z"
    },
    "id": "86f761ae87b4d33c"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# a shortcut for getting all the stimulus ids as a numpy array\n",
    "stimulus_ids = time_average['stimulus_id'].values\n",
    "\n",
    "# use 80% for training, 20% for validation\n",
    "# reference: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "train_stimuli, val_stimuli = '''%% use train_test_split to divide the stimulus_ids into 80% training and 20% validation, use random_state=3 %%'''\n",
    "\n",
    "# hint: for the following, use the multi-slicing technique we demonstrated earlier\n",
    "train_data = '''%% Write the code to select the training data using train_stimuli here %%'''\n",
    "val_data = '''%% Write the code to select the validation data using val_stimuli here %%'''\n",
    "\n",
    "# %% Reminder: After this, you should have train_data and val_data that are ready to be used for further analysis or model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2d992e7c5fb3b5",
   "metadata": {
    "id": "cb2d992e7c5fb3b5"
   },
   "source": [
    "What's chance accuracy on this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36615056e5a15d21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T12:55:05.978975Z",
     "start_time": "2025-02-19T12:55:05.964918Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36615056e5a15d21",
    "outputId": "47713a16-e4c1-4c5d-e306-bc3b885a2d93"
   },
   "outputs": [],
   "source": [
    "from numpy.random import RandomState\n",
    "\n",
    "train_y = train_data['texture_type'].values\n",
    "random_baseline = RandomState(seed=42).choice(train_y, replace=False, size=len(train_y))\n",
    "chance_accuracy = (random_baseline == train_y).mean()\n",
    "print(\"chance performance is\", chance_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1b088725cae54c",
   "metadata": {
    "id": "ee1b088725cae54c"
   },
   "source": [
    "With `random_state=3` for `train_test_split`, the chance is actually exactly 50% in this specific case; depending on randomness one run of this might give values slightly below/above 0.5.  \n",
    "\n",
    "Now we train a linear regression from the train brain data to the train stimulus types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05b031949b01a5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T12:55:08.021698Z",
     "start_time": "2025-02-19T12:55:05.978975Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f05b031949b01a5d",
    "outputId": "91484b93-fc8a-42da-a2c5-0185e4097153"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Reference: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "linear_readout = LogisticRegression()\n",
    "\n",
    "# reshape the training data appropriately by transposing the 'presentation' and 'neuroid_id' axes.\n",
    "# then, use .values to access the numpy array\n",
    "train_x = '''%% Write the code to extract the training features (train_x) here %%'''\n",
    "train_y = '''%% Write the code to extract the target labels (train_y) here %%'''\n",
    "\n",
    "# normalize data\n",
    "scaler = StandardScaler().fit(train_x)\n",
    "train_x = scaler.transform(train_x)\n",
    "\n",
    "# fit the decoder\n",
    "linear_readout.fit(train_x, train_y)\n",
    "\n",
    "# double-checking that the fitting worked\n",
    "train_predictions = linear_readout.predict(train_x)\n",
    "train_accuracy = (train_predictions == train_y).mean()\n",
    "print(\"train accuracy is\", train_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eeb2ab6c4f2a9f",
   "metadata": {
    "id": "77eeb2ab6c4f2a9f"
   },
   "source": [
    "Great, we were able to (over)fit the training data. How well does this work on the validation data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd45949dbd49eec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-19T12:55:09.786149Z",
     "start_time": "2025-02-19T12:55:08.021698Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dcd45949dbd49eec",
    "outputId": "d1210ee0-9b0e-443e-8484-e84e1fa0298f"
   },
   "outputs": [],
   "source": [
    "val_x = '''%% Write the code to extract the validation features (val_x) here %%'''\n",
    "val_y = '''%% Write the code to extract the target labels (val_y) here %%'''\n",
    "\n",
    "# don't forget to normalize the inputs again\n",
    "val_x = scaler.transform(val_x)\n",
    "\n",
    "# evaluate the decoder on the validation set\n",
    "val_accuracy = linear_readout.score(val_x, val_y)\n",
    "print(\"validation accuracy is\", val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa16660aaa27376e",
   "metadata": {
    "id": "aa16660aaa27376e"
   },
   "source": [
    "Not so bad.\n",
    "\n",
    "Why did we not get 100% validation accuracy?\n",
    "\n",
    "How could you improve the accuracy of the classifier?\n",
    "\n",
    "Taking the [paper](https://www.nature.com/articles/nn.3402)'s Figure 2 into account, do you think the classifier performance with V2 data would be different than with V1?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
